{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
        "df = pd.read_csv(url, header=None)\n",
        "\n",
        "print(df.head)\n",
        "\n",
        "# Define feature and target variables\n",
        "X = df.iloc[:, :-1].values\n",
        "y = df.iloc[:, -1].values\n",
        "\n",
        "# Encode target labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "_SI-_bwSEanA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eff7f8cc-1a09-4518-eeca-826528cd3c46"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method NDFrame.head of        0    1    2    3               4\n",
            "0    5.1  3.5  1.4  0.2     Iris-setosa\n",
            "1    4.9  3.0  1.4  0.2     Iris-setosa\n",
            "2    4.7  3.2  1.3  0.2     Iris-setosa\n",
            "3    4.6  3.1  1.5  0.2     Iris-setosa\n",
            "4    5.0  3.6  1.4  0.2     Iris-setosa\n",
            "..   ...  ...  ...  ...             ...\n",
            "145  6.7  3.0  5.2  2.3  Iris-virginica\n",
            "146  6.3  2.5  5.0  1.9  Iris-virginica\n",
            "147  6.5  3.0  5.2  2.0  Iris-virginica\n",
            "148  6.2  3.4  5.4  2.3  Iris-virginica\n",
            "149  5.9  3.0  5.1  1.8  Iris-virginica\n",
            "\n",
            "[150 rows x 5 columns]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusions:\n",
        "\n",
        "I choose Iris dataset from the UCI repository. I made label encoding to convert categorical variables to numerical variables. I split the data into training and testing sets. I used 20% of the data for testing. I applied standart scaler to standart the features.\n"
      ],
      "metadata": {
        "id": "6MdxyPiia3S5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PART 2**"
      ],
      "metadata": {
        "id": "HhUMzWokZWlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikeras"
      ],
      "metadata": {
        "id": "sk48soOlcDKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "\n",
        "def create_mlp():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(10, input_dim=4, activation='relu'))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Wrap the Keras model for use in scikit-learn\n",
        "mlp = KerasClassifier(model=create_mlp, epochs=50, batch_size=5, verbose=0)\n",
        "\n",
        "print(mlp)\n"
      ],
      "metadata": {
        "id": "6yftZsAtbP4x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39fdbfbc-b14b-4102-f444-b72befc1c944"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KerasClassifier(\n",
            "\tmodel=<function create_mlp at 0x792907602ef0>\n",
            "\tbuild_fn=None\n",
            "\twarm_start=False\n",
            "\trandom_state=None\n",
            "\toptimizer=rmsprop\n",
            "\tloss=None\n",
            "\tmetrics=None\n",
            "\tbatch_size=5\n",
            "\tvalidation_batch_size=None\n",
            "\tverbose=0\n",
            "\tcallbacks=None\n",
            "\tvalidation_split=0.0\n",
            "\tshuffle=True\n",
            "\trun_eagerly=False\n",
            "\tepochs=50\n",
            "\tclass_weight=None\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "# Initialize AdaBoost with the MLP classifier as the base estimator\n",
        "ada_boost = AdaBoostClassifier(estimator=mlp, n_estimators=50, learning_rate=1.0)\n",
        "\n",
        "\n",
        "# Train AdaBoost\n",
        "ada_boost.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "97jM_9R9cR2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = ada_boost.predict(X_test)\n",
        "\n",
        "# Evaluate the performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Classification Report:\\n\", report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT8qCNUIdnvx",
        "outputId": "3803b5a1-f762-4f2c-f632-e7cc52914942"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9666666666666667\n",
            "Classification Report:\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "    Iris-setosa       1.00      1.00      1.00        11\n",
            "Iris-versicolor       0.86      1.00      0.92         6\n",
            " Iris-virginica       1.00      0.92      0.96        13\n",
            "\n",
            "       accuracy                           0.97        30\n",
            "      macro avg       0.95      0.97      0.96        30\n",
            "   weighted avg       0.97      0.97      0.97        30\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusions:\n",
        "\n",
        "**create_mlp** function defines the structure of the multi-layer perceptron. **Sequential** indicates a linear stack of layers.\n",
        "**Dense(10, input_dim=4, activation='relu')** adds a dense (fully connected) layer with 10 neurons, input dimension of 4 (since the Iris dataset has 4 features), and ReLU activation function.\n",
        "**Dense(3, activation='softmax')** adds an output layer with 3 neurons (since there are 3 classes in the Iris dataset) and softmax activation for multi-class classification.\n",
        "**compile** method specifies the optimizer (adam), loss function (sparse_categorical_crossentropy), and evaluation metric (accuracy).\n",
        "\n",
        "Then, I used **KerasClassifier** to create a simple multi-layer perceptron (MLP) with one hidden layer.\n",
        "\n",
        "*Keras models can be easily wrapped for use in scikit-learn with the KerasClassifier wrapper from scikeras. This integration allows the Keras model to be used as a base estimator in scikit-learn's ensemble methods, like AdaBoost in this case.*\n",
        "\n",
        "Then I used this mlp as the base classifier for the **AdaBoost** ensemble and trained the model.\n",
        "\n",
        "Finally I calculated the accuracy and report the performance with **classification_report**.\n"
      ],
      "metadata": {
        "id": "VaD87PGVcXQN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PART 3**"
      ],
      "metadata": {
        "id": "GrYhC8YRfXE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_perceptron():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(1, input_dim=4, activation='linear'))\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "fYoJhiY7eNMR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "import numpy as np\n",
        "\n",
        "class PerceptronTree(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, depth=1):\n",
        "        self.depth = depth\n",
        "        self.model = KerasRegressor(model=create_perceptron, epochs=10, batch_size=5, verbose=0)\n",
        "        self.left = None\n",
        "        self.right = None\n",
        "        self.is_leaf = True\n",
        "        self.label = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        if self.depth > 1 and len(np.unique(y)) > 1:\n",
        "            self.model.fit(X, y)\n",
        "            self.is_leaf = False\n",
        "            predictions = self.model.predict(X)\n",
        "            median = np.median(predictions)\n",
        "            left_indices = predictions <= median\n",
        "            right_indices = predictions > median\n",
        "            self.left = PerceptronTree(depth=self.depth - 1)\n",
        "            self.right = PerceptronTree(depth=self.depth - 1)\n",
        "            self.left.fit(X[left_indices], y[left_indices])\n",
        "            self.right.fit(X[right_indices], y[right_indices])\n",
        "        else:\n",
        "            self.is_leaf = True\n",
        "            self.label = np.argmax(np.bincount(y))\n",
        "\n",
        "    def predict(self, X):\n",
        "        if self.is_leaf:\n",
        "            return np.full(X.shape[0], self.label)\n",
        "        else:\n",
        "            predictions = self.model.predict(X)\n",
        "            median = np.median(predictions)\n",
        "            left_indices = predictions <= median\n",
        "            right_indices = predictions > median\n",
        "            y_pred = np.zeros(X.shape[0])\n",
        "            y_pred[left_indices] = self.left.predict(X[left_indices])\n",
        "            y_pred[right_indices] = self.right.predict(X[right_indices])\n",
        "            return y_pred\n"
      ],
      "metadata": {
        "id": "ajM6ZXjUeScr"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PerceptronForest(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, n_estimators=10, max_depth=3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.trees = [PerceptronTree(depth=max_depth) for _ in range(n_estimators)]\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for tree in self.trees:\n",
        "            indices = np.random.choice(X.shape[0], X.shape[0], replace=True)\n",
        "            tree.fit(X[indices], y[indices])\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = np.zeros((self.n_estimators, X.shape[0]))\n",
        "        for i, tree in enumerate(self.trees):\n",
        "            predictions[i] = tree.predict(X)\n",
        "        return np.round(np.mean(predictions, axis=0))\n"
      ],
      "metadata": {
        "id": "qnAvykrUerc3"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import tensorflow as tf\n",
        "\n",
        "# Set random seed for determinism\n",
        "tf.random.set_seed(1)\n",
        "\n",
        "# Initialize and train the perceptron forest\n",
        "perceptron_forest = PerceptronForest(n_estimators=10, max_depth=3)\n",
        "perceptron_forest.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = perceptron_forest.predict(X_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "n3jLCKEjeuHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Classification Report:\\n\", report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3wtDAzohOWf",
        "outputId": "4230f39a-8880-4436-d7de-ed3aaeb93b17"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8\n",
            "Classification Report:\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "    Iris-setosa       1.00      1.00      1.00        11\n",
            "Iris-versicolor       0.50      1.00      0.67         6\n",
            " Iris-virginica       1.00      0.54      0.70        13\n",
            "\n",
            "       accuracy                           0.80        30\n",
            "      macro avg       0.83      0.85      0.79        30\n",
            "   weighted avg       0.90      0.80      0.80        30\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusions:\n",
        "\n",
        "I implemented a class called **PerceptronForest** which constructs a forest of decision trees.\n",
        "\n",
        "I defined a class called **PerceptronTree** which represents *a decision tree where each node is a perceptron*. I used the **KerasRegressor** wrapper from scikeras to integrate the perceptron.\n",
        "\n",
        "In the fit method of PerceptronTree, I trained a perceptron at each node to predict the target. Then, I split the data based on the predictions of the perceptron. The split was determined by the median of the perceptron's predictions. In the predict method of PerceptronTree, I used the perceptron's predictions to decide which branch to follow for each input sample.\n",
        "\n",
        "Finally I calculated the accuracy and report the performance with classification_report."
      ],
      "metadata": {
        "id": "088RFS9Nkdw5"
      }
    }
  ]
}